{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import jieba\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data_path = 'data/atec_nlp_sim_train.csv'  # 训练数据\n",
    "train_add_data_path = 'data/atec_nlp_sim_train_add.csv'  # 添加训练数据\n",
    "stop_words_path = 'data/stop_words.txt'  # 停用词路径\n",
    "tokenize_dict_path = 'data/dict_all.txt'  # jieba分词新自定义字典\n",
    "spelling_corrections_path = 'data/spelling_corrections.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data_df = pd.read_csv(train_data_path, sep='\\t', header=None,names=[\"index\", \"s1\", \"s2\", \"label\"])\n",
    "train_add_data_df = pd.read_csv(train_add_data_path, sep='\\t', header=None, names=[\"index\", \"s1\", \"s2\", \"label\"])\n",
    "train_all = pd.concat([train_data_df, train_add_data_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_all = train_all.sample(frac=1).reset_index(drop=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>﻿怎么更改花呗手机号码</td>\n",
       "      <td>我的花呗是以前的手机号码，怎么更改成现在的支付宝的号码手机号</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>也开不了花呗，就这样了？完事了</td>\n",
       "      <td>真的嘛？就是花呗付款</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>花呗冻结以后还能开通吗</td>\n",
       "      <td>我的条件可以开通花呗借款吗</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>如何得知关闭借呗</td>\n",
       "      <td>想永久关闭借呗</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>花呗扫码付钱</td>\n",
       "      <td>二维码扫描可以用花呗吗</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index               s1                              s2  label\n",
       "0      1      ﻿怎么更改花呗手机号码  我的花呗是以前的手机号码，怎么更改成现在的支付宝的号码手机号      1\n",
       "1      2  也开不了花呗，就这样了？完事了                      真的嘛？就是花呗付款      0\n",
       "2      3      花呗冻结以后还能开通吗                   我的条件可以开通花呗借款吗      0\n",
       "3      4         如何得知关闭借呗                         想永久关闭借呗      0\n",
       "4      5           花呗扫码付钱                     二维码扫描可以用花呗吗      0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_all.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分词及处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\zxq\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.851 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "jieba.load_userdict(tokenize_dict_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 停用词表\n",
    "stopwords = [line.strip() for line in open(stop_words_path, 'r', encoding='utf-8').readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 拼错词替换表\n",
    "with open(spelling_corrections_path,\"r\",encoding=\"utf-8\") as file:\n",
    "    spelling_corrections = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transform_other_word(str_text,reg_dict):\n",
    "    \"\"\"\n",
    "    替换词\n",
    "    :param str_text:待替换的句子\n",
    "    :param reg_dict:替换词字典\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    for token_str,replac_str in reg_dict.items():\n",
    "        str_text = str_text.replace(token_str, replac_str)\n",
    "    return str_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def seg_sentence(sentence, stop_words):\n",
    "    \"\"\"\n",
    "    对句子进行分词\n",
    "    :param sentence:句子，停用词\n",
    "    \"\"\"\n",
    "    sentence_seged = jieba.cut(sentence.strip())\n",
    "    word_list = [i for i in sentence_seged if i not in stop_words and i != ' ']\n",
    "    return \" \".join(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocessing_word(s1_train, s2_train, stopwords, spelling_corrections):\n",
    "\n",
    "    # 去除句子中的脱敏数字***，替换成一\n",
    "    re_object = re.compile(r'\\*+')\n",
    "\n",
    "    s1_all = []\n",
    "    s2_all = []\n",
    "    all = []\n",
    "\n",
    "    for s1_,s2_ in zip(s1_train, s2_train):\n",
    "        s1 = re_object.sub(u\"十一\", s1_)\n",
    "        s2 = re_object.sub(u\"十一\", s2_)\n",
    "        spell_corr_s1 = transform_other_word(s1, spelling_corrections)\n",
    "        spell_corr_s2 = transform_other_word(s2, spelling_corrections)\n",
    "\n",
    "        # 分词\n",
    "        seg_s1 = seg_sentence(spell_corr_s1, stopwords)\n",
    "        seg_s2 = seg_sentence(spell_corr_s2, stopwords)\n",
    "\n",
    "        all.extend(seg_s1)\n",
    "        all.extend(seg_s2)\n",
    "        s1_all.append(seg_s1)\n",
    "        s2_all.append(seg_s2)\n",
    "    source_list = []\n",
    "    # source_list = list(set(all))\n",
    "    source_list.append('<UNK>')\n",
    "    source_list.append('<PAD>')\n",
    "    source_list.extend(list(set(all)))\n",
    "    word2id = {}\n",
    "    id2word = {}\n",
    "    for index, char in enumerate(source_list):\n",
    "        word2id[char] = index\n",
    "        id2word[index] = char\n",
    "\n",
    "    return s1_all, s2_all, word2id, id2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s1_train = train_all[\"s1\"].tolist()\n",
    "s2_train = train_all[\"s2\"].tolist()\n",
    "y_train = train_all[\"label\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s1_word_all, s2_word_all, word2id, id2word = preprocessing_word(s1_train, s2_train, stopwords, spelling_corrections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_word2id(data, word2id):\n",
    "    data2id = []\n",
    "    for word_list in data:\n",
    "        id_list = [word2id.get(i) if word2id.get(i) is not None else word2id.get('<PAD>') for i in word_list]\n",
    "        data2id.append(id_list)\n",
    "    return data2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def all_data_set(s1_all, s2_all, word2id, y_train, max_l=15):\n",
    "    pad = word2id['<PAD>']\n",
    "    all_data = []\n",
    "    s1_data_id = make_word2id(s1_all, word2id)\n",
    "    s2_data_id = make_word2id(s2_all, word2id)\n",
    "    s1_all_new = []\n",
    "    s2_all_new = []\n",
    "    y = []\n",
    "    for i in range(len(s1_data_id)):\n",
    "        if len(s1_data_id[i]) > max_l:\n",
    "            s1_set = s1_data_id[i][:max_l]\n",
    "        else:\n",
    "            s1_set = np.concatenate((s1_data_id[i], np.tile(pad, max_l - len(s1_data_id[i]))), axis=0)\n",
    "        if len(s2_data_id[i]) > max_l:\n",
    "            s2_set = s2_data_id[i][:max_l]\n",
    "        else:\n",
    "            s2_set = np.concatenate((s2_data_id[i], np.tile(pad, max_l - len(s2_data_id[i]))), axis=0)\n",
    "        y_set = [1,0] if y_train[i] == 0 else [0,1]\n",
    "        s1_all_new.append(s1_set)\n",
    "        s2_all_new.append(s2_set)\n",
    "        y.append(y_set)\n",
    "    return s1_all_new, s2_all_new, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s1_word_id_all, s2_word_id_all, y_set = all_data_set(s1_word_all, s2_word_all, word2id, y_train, max_l=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_all[\"s1\"] = s1_word_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_all[\"s2\"] = s2_word_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_all[\"s1_word_id\"] = s1_word_id_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_all[\"s2_word_id\"] = s2_word_id_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_all[\"y_set\"] = y_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th>label</th>\n",
       "      <th>s1_word_id</th>\n",
       "      <th>s2_word_id</th>\n",
       "      <th>y_set</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>﻿ 怎么 更换 花呗 手机号码</td>\n",
       "      <td>花呗 是 以前 手机号码 怎么 更换 成 现在 支付宝 号码 手机号</td>\n",
       "      <td>1</td>\n",
       "      <td>[2106, 1421, 2057, 511, 1421, 1521, 908, 1421,...</td>\n",
       "      <td>[245, 563, 1421, 1280, 1421, 849, 1929, 1421, ...</td>\n",
       "      <td>[0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>开不了 花呗 这样 完事</td>\n",
       "      <td>真的 就是 花呗 付款</td>\n",
       "      <td>0</td>\n",
       "      <td>[771, 1642, 1732, 1421, 245, 563, 1421, 42, 12...</td>\n",
       "      <td>[613, 1407, 1421, 760, 1280, 1421, 245, 563, 1...</td>\n",
       "      <td>[1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>花呗 冻结 以后 能 开通</td>\n",
       "      <td>条件 可以 开通 花呗 借款</td>\n",
       "      <td>0</td>\n",
       "      <td>[245, 563, 1421, 883, 724, 1421, 849, 1436, 14...</td>\n",
       "      <td>[335, 312, 1421, 2005, 849, 1421, 771, 181, 14...</td>\n",
       "      <td>[1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>如何 得知 关 借呗</td>\n",
       "      <td>永久 关 借呗</td>\n",
       "      <td>0</td>\n",
       "      <td>[939, 517, 1421, 158, 463, 1421, 922, 1421, 20...</td>\n",
       "      <td>[1868, 385, 1421, 922, 1421, 2088, 563, 1, 1, ...</td>\n",
       "      <td>[1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>花呗 扫码 付钱</td>\n",
       "      <td>二维码 扫描 可以 用花呗</td>\n",
       "      <td>0</td>\n",
       "      <td>[245, 563, 1421, 938, 753, 1421, 50, 1542, 1, ...</td>\n",
       "      <td>[861, 615, 753, 1421, 938, 186, 1421, 2005, 84...</td>\n",
       "      <td>[1, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index               s1                                  s2  label  \\\n",
       "0      1  ﻿ 怎么 更换 花呗 手机号码  花呗 是 以前 手机号码 怎么 更换 成 现在 支付宝 号码 手机号      1   \n",
       "1      2     开不了 花呗 这样 完事                         真的 就是 花呗 付款      0   \n",
       "2      3    花呗 冻结 以后 能 开通                      条件 可以 开通 花呗 借款      0   \n",
       "3      4       如何 得知 关 借呗                             永久 关 借呗      0   \n",
       "4      5         花呗 扫码 付钱                       二维码 扫描 可以 用花呗      0   \n",
       "\n",
       "                                          s1_word_id  \\\n",
       "0  [2106, 1421, 2057, 511, 1421, 1521, 908, 1421,...   \n",
       "1  [771, 1642, 1732, 1421, 245, 563, 1421, 42, 12...   \n",
       "2  [245, 563, 1421, 883, 724, 1421, 849, 1436, 14...   \n",
       "3  [939, 517, 1421, 158, 463, 1421, 922, 1421, 20...   \n",
       "4  [245, 563, 1421, 938, 753, 1421, 50, 1542, 1, ...   \n",
       "\n",
       "                                          s2_word_id   y_set  \n",
       "0  [245, 563, 1421, 1280, 1421, 849, 1929, 1421, ...  [0, 1]  \n",
       "1  [613, 1407, 1421, 760, 1280, 1421, 245, 563, 1...  [1, 0]  \n",
       "2  [335, 312, 1421, 2005, 849, 1421, 771, 181, 14...  [1, 0]  \n",
       "3  [1868, 385, 1421, 922, 1421, 2088, 563, 1, 1, ...  [1, 0]  \n",
       "4  [861, 615, 753, 1421, 938, 186, 1421, 2005, 84...  [1, 0]  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 词向量路径\n",
    "train_all_wordvec_path = \"data/train_all_data.bigram\"           #全部数据训练的词向量\n",
    "train_char_all_wordvec_path = \"data/train_char_all_data.bigram\" #全部数据训练的词向量\n",
    "zhihu_wordvec_path = \"data/sgns.zhihu.bigram\"                   #知乎词向量\n",
    "doubt_words_path = 'data/doubt_words.txt'     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#字符特征提取\n",
    "\n",
    "#抽取两个句子长度之差(归一化)\n",
    "def extract_sentece_length_diff(train_all):\n",
    "    \"\"\"\n",
    "    长度差特征\n",
    "    \"\"\"\n",
    "    feature_train = np.zeros((train_all.shape[0],1),dtype='float32')\n",
    "\n",
    "    # 计算两个句子的长度差\n",
    "    def get_length_diff(s1, s2):\n",
    "        return 1 - (abs(len(s1) - len(s2)) / float(max(len(s1), len(s2))))\n",
    "\n",
    "    for index,row in train_all.iterrows():\n",
    "        s1 = row['s1'].strip().split(' ')\n",
    "        s2 = row['s2'].strip().split(' ')\n",
    "        diff = get_length_diff(s1,s2)\n",
    "        feature_train[index] = round(diff,5)\n",
    "\n",
    "    return feature_train\n",
    "\n",
    "#抽取两个句子编辑距离(归一化)\n",
    "def extract_edit_distance(train_all):\n",
    "    \"\"\"\n",
    "    编辑距离特征\n",
    "    \"\"\"\n",
    "    feature_train = np.zeros((train_all.shape[0], 1), dtype='float32')\n",
    "\n",
    "    # 计算编辑距离\n",
    "    def get_edit_distance(rawq1, rawq2):\n",
    "        #构建DP矩阵\n",
    "        m, n = len(rawq1) + 1, len(rawq2) + 1\n",
    "        matrix = [[0] * n for i in range(m)]\n",
    "        matrix[0][0] = 0\n",
    "        for i in range(1, m):\n",
    "            matrix[i][0] = matrix[i - 1][0] + 1\n",
    "        for j in range(1, n):\n",
    "            matrix[0][j] = matrix[0][j - 1] + 1\n",
    "        cost = 0\n",
    "        for i in range(1, m):\n",
    "            for j in range(1, n):\n",
    "                if rawq1[i - 1] == rawq2[j - 1]:\n",
    "                    cost = 0\n",
    "                else:\n",
    "                    cost = 1\n",
    "                matrix[i][j] = min(matrix[i - 1][j] + 1, matrix[i][j - 1] + 1, matrix[i - 1][j - 1] + cost)\n",
    "        return 1 - (matrix[m - 1][n - 1] / float(max(len(rawq1), len(rawq2))))\n",
    "\n",
    "    for index,row in train_all.iterrows():\n",
    "        s1 = row['s1'].strip()\n",
    "        s2 = row['s2'].strip()\n",
    "        edit_distance = get_edit_distance(s1,s2)\n",
    "        feature_train[index] = round(edit_distance,5)\n",
    "\n",
    "    return feature_train\n",
    "\n",
    "#抽取公共子串特征\n",
    "def extract_longest_common_substring(train_all):\n",
    "    \"\"\"\n",
    "    公共子串特征\n",
    "    \"\"\"\n",
    "    feature_train = np.zeros((train_all.shape[0], 1), dtype='float32')\n",
    "\n",
    "    # 计算最长公共子串\n",
    "    def get_common_substring_len(rawq1, rawq2):\n",
    "        #构建DP矩阵\n",
    "        m, n = len(rawq1) + 1, len(rawq2) + 1\n",
    "        matrix = [[0] * n for i in range(m)]\n",
    "        longest_num = 0\n",
    "        for i in range(1, m):\n",
    "            for j in range(1, n):\n",
    "                if rawq1[i - 1] == rawq2[j - 1]:\n",
    "                    matrix[i][j] = matrix[i-1][j-1] + 1\n",
    "                    if matrix[i][j] > longest_num:\n",
    "                        longest_num = matrix[i][j]\n",
    "                    else:\n",
    "                        matrix[i][j] = 0\n",
    "        return longest_num / float(min(len(rawq1), len(rawq2)))\n",
    "    for index,row in train_all.iterrows():\n",
    "        s1 = row['s1'].strip()\n",
    "        s2 = row['s2'].strip()\n",
    "        common_substring_len = get_common_substring_len(s1,s2)\n",
    "        feature_train[index] = round(common_substring_len,5)\n",
    "\n",
    "    return feature_train\n",
    "\n",
    "#抽取公共子序列特征\n",
    "def extract_longest_common_subsequence(train_all):\n",
    "    \"\"\"\n",
    "    公共子序列特征\n",
    "    \"\"\"\n",
    "    feature_train = np.zeros((train_all.shape[0], 1), dtype='float32')\n",
    "\n",
    "    # 计算最长公共子序列\n",
    "    def get_common_subsequence_len(rawq1, rawq2):\n",
    "        #构建DP矩阵\n",
    "        m, n = len(rawq1) + 1, len(rawq2) + 1\n",
    "        matrix = [[0] * n for i in range(m)]\n",
    "        for i in range(1, m):\n",
    "            for j in range(1, n):\n",
    "                if rawq1[i - 1] == rawq2[j - 1]:\n",
    "                    matrix[i][j] = matrix[i-1][j-1] + 1\n",
    "                else:\n",
    "                    matrix[i][j] = max(matrix[i-1][j],matrix[i][j-1])\n",
    "        return matrix[m-1][n-1] / float(min(len(rawq1), len(rawq2)))\n",
    "    for index,row in train_all.iterrows():\n",
    "        s1 = row['s1'].strip()\n",
    "        s2 = row['s2'].strip()\n",
    "        common_subsequence_len = get_common_subsequence_len(s1,s2)\n",
    "        feature_train[index] = round(common_subsequence_len,5)\n",
    "\n",
    "    return feature_train\n",
    "\n",
    "\n",
    "#抽取n-gram特征，计算两个句子n-gram下的差异\n",
    "def extract_ngram(train_all,max_ngram = 3):\n",
    "    '''\n",
    "    提取ngram特征\n",
    "    '''\n",
    "    feature_train = np.zeros((train_all.shape[0], max_ngram), dtype='float32')\n",
    "\n",
    "    # 提取n-gram词汇\n",
    "    def get_ngram(rawq, ngram_value):\n",
    "        result = []\n",
    "        for i in range(len(rawq)):\n",
    "            if i + ngram_value < len(rawq) + 1:\n",
    "                result.append(rawq[i:i + ngram_value])\n",
    "        return result\n",
    "\n",
    "    #提取两个句子词的差异（归一化）\n",
    "    def get_ngram_sim(q1_ngram, q2_ngram):\n",
    "        q1_dict = {}\n",
    "        q2_dict = {}\n",
    "\n",
    "        #统计q1_ngram中个词汇的个数\n",
    "        for token in q1_ngram:\n",
    "            if token not in q1_dict:\n",
    "                q1_dict[token] = 1\n",
    "            else:\n",
    "                q1_dict[token] = q1_dict[token] + 1\n",
    "        #q1_ngram总词汇数\n",
    "        q1_count = np.sum([value for key, value in q1_dict.items()])\n",
    "\n",
    "        #统计q2_ngram中个词汇的个数\n",
    "        for token in q2_ngram:\n",
    "            if token not in q2_dict:\n",
    "                q2_dict[token] = 1\n",
    "            else:\n",
    "                q2_dict[token] = q2_dict[token] + 1\n",
    "        #q2_ngram总词汇数\n",
    "        q2_count = np.sum([value for key, value in q2_dict.items()])\n",
    "\n",
    "        # ngram1有但是ngram2没有\n",
    "        q1_count_only = np.sum([value for key, value in q1_dict.items() if key not in q2_dict])\n",
    "        # ngram2有但是ngram1没有\n",
    "        q2_count_only = np.sum([value for key, value in q2_dict.items() if key not in q1_dict])\n",
    "        # ngram1和ngram2都有的话，计算value的差值\n",
    "        q1_q2_count = np.sum([abs(value - q2_dict[key]) for key, value in q1_dict.items() if key in q2_dict])\n",
    "        # ngram1和ngram2的总值\n",
    "        all_count = q1_count + q2_count\n",
    "\n",
    "        return (1 - float(q1_count_only + q2_count_only + q1_q2_count) / (float(all_count) + 0.00000001))\n",
    "\n",
    "    for ngram_value in range(max_ngram):\n",
    "        for index, row in train_all.iterrows():\n",
    "            s1 = row['s1'].strip()\n",
    "            s2 = row['s2'].strip()\n",
    "            ngram1 = get_ngram(s1, ngram_value + 1)\n",
    "            ngram2 = get_ngram(s2, ngram_value + 1)\n",
    "            ngram_sim = get_ngram_sim(ngram1, ngram2)\n",
    "            feature_train[index,ngram_value] = round(ngram_sim,5)\n",
    "\n",
    "    return feature_train\n",
    "\n",
    "\n",
    "#抽取两个句子的 相同字的长度/较长句子长度、相同字的长度/较短句子长度、相同字的长度/两句子平均长度、句子1中独有字的长度/句子1长度、句子2中独有字的长度/句子2长度、两个句子的杰卡德距离\n",
    "def extract_sentence_diff_same(train_all):\n",
    "    '''\n",
    "    两个句子的相同和不同的词特征\n",
    "    '''\n",
    "    col_num = 6\n",
    "    feature_train = np.zeros((train_all.shape[0],col_num),dtype='float64')\n",
    "\n",
    "    #统计两个句子的相同和不同\n",
    "    def get_word_diff(q1, q2):\n",
    "        set1 = set(q1.split(\" \"))\n",
    "        set2 = set(q2.split(\" \"))\n",
    "\n",
    "        #两个句子相同词的长度\n",
    "        same_word_len = len(set1 & set2)\n",
    "\n",
    "        #仅句子1中有的词汇个数\n",
    "        unique_word1_len = len(set1 - set2)\n",
    "\n",
    "        #仅句子2中有的词汇个数\n",
    "        unique_word2_len = len(set2 - set1)\n",
    "\n",
    "        #句子1中词汇个数\n",
    "        word1_len = len(set1)\n",
    "\n",
    "        #句子2中词汇个数\n",
    "        word2_len = len(set2)\n",
    "\n",
    "        #两句子的平均长度\n",
    "        avg_len = (word1_len + word2_len) / 2.0\n",
    "\n",
    "        #两个句子中较长的长度\n",
    "        max_len = max(word1_len, word2_len)\n",
    "\n",
    "        #两个句子中较短的长度\n",
    "        min_len = min(word1_len, word2_len)\n",
    "\n",
    "        #两个句子的杰卡德距离\n",
    "        jaccard_sim = same_word_len / float(len(set1 | set2))\n",
    "\n",
    "        return same_word_len / float(max_len), same_word_len / float(min_len), same_word_len / float(avg_len), \\\n",
    "               unique_word1_len / float(word1_len), unique_word2_len /float(word2_len), jaccard_sim\n",
    "\n",
    "    for index,row in train_all.iterrows():\n",
    "        s1 = row['s1'].strip()\n",
    "        s2 = row['s2'].strip()\n",
    "        features = tuple()\n",
    "        features = get_word_diff(s1,s2)\n",
    "        for col_index,feature in enumerate(features):\n",
    "            feature_train[index,col_index] = round(feature,5)\n",
    "\n",
    "    return feature_train\n",
    "\n",
    "#抽取疑问词相同的比例\n",
    "def extract_doubt_sim(train_all):\n",
    "    '''\n",
    "    抽取疑问词相同的比例\n",
    "    '''\n",
    "    feature_train = np.zeros((train_all.shape[0], 1), dtype='float32')\n",
    "\n",
    "    with open(doubt_words_path,\"r\",encoding=\"utf-8\") as file:\n",
    "        doubt_words = [line.strip() for line in file]\n",
    "\n",
    "    # 获取疑问词相同的比例\n",
    "    def get_doubt_sim(q1, q2, doubt_words):\n",
    "        q1_doubt_words = set(q1.split(\" \")) & set(doubt_words)\n",
    "        q2_doubt_words = set(q2.split(\" \")) & set(doubt_words)\n",
    "        return len(q1_doubt_words & q2_doubt_words) / float(len(q1_doubt_words | q2_doubt_words) + 1)\n",
    "\n",
    "    for index,row in train_all.iterrows():\n",
    "        # 因为doubt_words词表加载出来的是Unicode，所以需要将s1,s2解码成Unicode\n",
    "        s1 = row['s1'].strip()\n",
    "        s2 = row['s2'].strip()\n",
    "        doubt_sim = get_doubt_sim(s1,s2,doubt_words)\n",
    "        feature_train[index] = round(doubt_sim,5)\n",
    "\n",
    "    return feature_train\n",
    "\n",
    "#抽取两个句子中是否同时存在蚂蚁花呗或者蚂蚁借呗的特征,同时包含花呗为1，同时包含借呗为1，否则为0\n",
    "def extract_sentence_exist_topic(train_all):\n",
    "    \"\"\"\n",
    "    抽取两个句子中是否同时存在蚂蚁花呗或者蚂蚁借呗的特征,同时包含花呗为1，同时包含借呗为1，否则为0\n",
    "    \"\"\"\n",
    "    with open(doubt_words_path,\"r\",encoding=\"utf-8\") as file:\n",
    "        doubt_words = [line.strip() for line in file]\n",
    "\n",
    "    feature_train = np.zeros((train_all.shape[0], 2), dtype='float32')\n",
    "\n",
    "    def get_exist_same_topic(rawq1,rawq2):\n",
    "        hua_flag = 0.\n",
    "        jie_flag = 0.\n",
    "        if '花呗' in rawq1 and '花呗' in rawq2:\n",
    "            hua_flag = 1.\n",
    "\n",
    "        if '借呗' in rawq1 and '借呗' in rawq2:\n",
    "            jie_flag = 1.\n",
    "\n",
    "        return hua_flag,jie_flag\n",
    "\n",
    "    for index,row in train_all.iterrows():\n",
    "        s1 = row['s1'].strip()\n",
    "        s2 = row['s2'].strip()\n",
    "        hua_flag, jie_flag = get_exist_same_topic(s1,s2)\n",
    "        feature_train[index,0] = hua_flag\n",
    "        feature_train[index,1] = jie_flag\n",
    "\n",
    "    return feature_train\n",
    "\n",
    "#提取句子的词向量组合的相似度\n",
    "def extract_word_embedding_sim(train_all,w2v_model_path = train_all_wordvec_path,extern_word2vec_path = zhihu_wordvec_path):\n",
    "    '''\n",
    "    提取句子的词向量组合的相似度\n",
    "    w2v_model_path为词向量文件\n",
    "    :return:\n",
    "    '''\n",
    "    #定义提取特征的空间\n",
    "    feature_train = np.zeros((train_all.shape[0], 1), dtype='float32')\n",
    "\n",
    "    train_all_w2v_model = KeyedVectors.load_word2vec_format(w2v_model_path, binary=False)\n",
    "\n",
    "    zhihu_w2v_model = KeyedVectors.load_word2vec_format(zhihu_wordvec_path, binary=False)\n",
    "\n",
    "    # 得到句子的词向量组合（tfidf）\n",
    "    def get_sen_vec(q, train_all_w2v_model, tfidf_dict, tfidf_flag=True):\n",
    "        sen_vec = 0\n",
    "        for word in q.split(' '):\n",
    "            if word in zhihu_w2v_model.vocab:\n",
    "                word_vec = zhihu_w2v_model.word_vec(word)\n",
    "                word_tfidf = tfidf_dict.get(word, None)\n",
    "                if tfidf_flag == True:\n",
    "                    #tfidf有效，词向量*tfidf权重=句子向量\n",
    "                    sen_vec += word_vec * word_tfidf\n",
    "                else:\n",
    "                    #句子向量\n",
    "                    sen_vec += word_vec\n",
    "            elif word in train_all_w2v_model.vocab:\n",
    "                word_vec = train_all_w2v_model.word_vec(word)\n",
    "                word_tfidf = tfidf_dict.get(word, None)\n",
    "\n",
    "                if tfidf_flag == True:\n",
    "                    #tfidf有效，词向量*tfidf权重=句子向量\n",
    "                    sen_vec += word_vec * word_tfidf\n",
    "                else:\n",
    "                    #句子向量\n",
    "                    sen_vec += word_vec\n",
    "\n",
    "            else:\n",
    "                pass\n",
    "        sen_vec = sen_vec / np.sqrt(np.sum(np.power(sen_vec, 2)) + 0.000001)\n",
    "        return sen_vec\n",
    "    def get_sentece_embedding_sim(q1, q2, train_all_w2v_model, tfidf_dict, tfidf_flag=True):\n",
    "        # 得到两个问句的词向量组合\n",
    "        q1_sec = get_sen_vec(q1, train_all_w2v_model, tfidf_dict, tfidf_flag)\n",
    "        q2_sec = get_sen_vec(q2, train_all_w2v_model, tfidf_dict, tfidf_flag)\n",
    "\n",
    "        # 曼哈顿距离\n",
    "        # manhattan_distance = np.sum(np.abs(np.subtract(q1_sec, q2_sec)))\n",
    "\n",
    "        # 欧式距离\n",
    "        # enclidean_distance = np.sqrt(np.sum(np.power((q1_sec - q2_sec),2)))\n",
    "\n",
    "        # 余弦相似度\n",
    "        molecular = np.sum(np.multiply(q1_sec, q2_sec))\n",
    "        denominator = np.sqrt(np.sum(np.power(q1_sec, 2))) * np.sqrt(np.sum(np.power(q2_sec, 2)))\n",
    "        cos_sim = molecular / (denominator + 0.000001)\n",
    "\n",
    "        # 闵可夫斯基距离\n",
    "        # minkowski_distance = np.power(np.sum(np.power(np.abs(np.subtract(q1_sec, q2_sec)), 3)), 0.333333)\n",
    "\n",
    "        # return manhattan_distance, enclidean_distance, cos_sim, minkowski_distance\n",
    "        return cos_sim\n",
    "\n",
    "    for index,row in train_all.iterrows():\n",
    "        s1 = row['s1'].strip()\n",
    "        s2 = row['s2'].strip()\n",
    "        sentece_embedding_sim = get_sentece_embedding_sim(s1,s2,train_all_w2v_model,{},False)\n",
    "        feature_train[index] = round(sentece_embedding_sim,5)\n",
    "\n",
    "    return feature_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentece_length_diff_feature = extract_sentece_length_diff(train_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(102477, 1)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentece_length_diff_feature.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "edit_distance_feature = extract_edit_distance(train_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(102477, 1)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edit_distance_feature.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "common_substring_feature = extract_longest_common_substring(train_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(102477, 1)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_substring_feature.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "common_subsequence_feature = extract_longest_common_subsequence(train_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(102477, 1)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_subsequence_feature.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ngram_feature = extract_ngram(train_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(102477, 3)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram_feature.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentence_diff_same_feature = extract_sentence_diff_same(train_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(102477, 6)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_diff_same_feature.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doubt_sim_feature = extract_doubt_sim(train_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(102477, 1)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doubt_sim_feature.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentence_exist_topic_feature = extract_sentence_exist_topic(train_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(102477, 2)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_exist_topic_feature.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zxq\\AppData\\Local\\conda\\conda\\envs\\zu\\lib\\site-packages\\gensim\\utils.py:1209: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_embedding_sim_feature = extract_word_embedding_sim(train_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(102477, 1)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_embedding_sim_feature.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "statistic_feature = np.concatenate([sentece_length_diff_feature,\n",
    "                            edit_distance_feature,\n",
    "                            common_substring_feature,\n",
    "                            common_subsequence_feature,\n",
    "                            ngram_feature,\n",
    "                            sentence_diff_same_feature,\n",
    "                            doubt_sim_feature,\n",
    "                            sentence_exist_topic_feature,\n",
    "                            word_embedding_sim_feature],\n",
    "                            axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(102477, 17)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "statistic_feature.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.45455   ,  0.29412001,  0.06667   ,  0.73333001,  0.57143003,\n",
       "        0.51064003,  0.40000001,  0.36364   ,  0.8       ,  0.5       ,\n",
       "        0.2       ,  0.63636   ,  0.33333   ,  0.5       ,  1.        ,\n",
       "        0.        ,  0.77051002])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "statistic_feature[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "102477"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 将特征存入pickle中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这个只是保存在pickle中了,需要用的时候调用pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 将数据存入pickle中\n",
    "with open(\"statistic_feature.pk\", 'wb') as f1:\n",
    "    pickle.dump(statistic_feature, f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 将数据存到一个大列表里面，格式是[[s1,s2,y],[s1,s2,y],[s1,s2,y].......]\n",
    "all_data = []\n",
    "for i in range(len(s1_word_id_all)):\n",
    "    all_data.append([s1_word_id_all[i],s2_word_id_all[i],y_set[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 将数据存入pickle中\n",
    "with open(\"word_data.pk\", 'wb') as f1:\n",
    "    pickle.dump((all_data,word2id,id2word), f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
