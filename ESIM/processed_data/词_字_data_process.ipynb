{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import jieba\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_path = '../data/atec_nlp_sim_train.csv'  # 训练数据\n",
    "train_add_data_path = '../data/atec_nlp_sim_train_add.csv'  # 添加训练数据\n",
    "stop_words_path = '../data/stop_words.txt'  # 停用词路径\n",
    "tokenize_dict_path = '../data/dict_all.txt'  # jieba分词新自定义字典\n",
    "spelling_corrections_path = '../data/spelling_corrections.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_df = pd.read_csv(train_data_path, sep='\\t', header=None,names=[\"index\", \"s1\", \"s2\", \"label\"])\n",
    "train_add_data_df = pd.read_csv(train_add_data_path, sep='\\t', header=None, names=[\"index\", \"s1\", \"s2\", \"label\"])\n",
    "train_all = pd.concat([train_data_df, train_add_data_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_all = train_all.sample(frac=1).reset_index(drop=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20460</td>\n",
       "      <td>为什么花呗一定要优先还本月</td>\n",
       "      <td>钱已退回花呗，为什么这个月还要还那么</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>32399</td>\n",
       "      <td>花呗分期后下一期还可以分期吗</td>\n",
       "      <td>花呗上个月分期后，这个月还可以分期吗</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4920</td>\n",
       "      <td>商家无法收花呗的付款</td>\n",
       "      <td>花呗支付完，剩余尾款无法确认付款了</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33433</td>\n",
       "      <td>蚂蚁借呗怎么提前还款下月的</td>\n",
       "      <td>借呗可以提前还当月的还款</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13667</td>\n",
       "      <td>为什么我这个帐号开不了花呗</td>\n",
       "      <td>以前手机号开通花呗，换个手机怎么就不能开通了</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index              s1                      s2  label\n",
       "0  20460   为什么花呗一定要优先还本月      钱已退回花呗，为什么这个月还要还那么      0\n",
       "1  32399  花呗分期后下一期还可以分期吗      花呗上个月分期后，这个月还可以分期吗      0\n",
       "2   4920      商家无法收花呗的付款       花呗支付完，剩余尾款无法确认付款了      0\n",
       "3  33433   蚂蚁借呗怎么提前还款下月的            借呗可以提前还当月的还款      0\n",
       "4  13667   为什么我这个帐号开不了花呗  以前手机号开通花呗，换个手机怎么就不能开通了      0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_all.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分词及处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache C:\\Users\\zxq\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 1.068 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "jieba.load_userdict(tokenize_dict_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 停用词表\n",
    "stopwords = [line.strip() for line in open(stop_words_path, 'r', encoding='utf-8').readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 拼错词替换表\n",
    "with open(spelling_corrections_path,\"r\",encoding=\"utf-8\") as file:\n",
    "    spelling_corrections = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_other_word(str_text,reg_dict):\n",
    "    \"\"\"\n",
    "    替换词\n",
    "    :param str_text:待替换的句子\n",
    "    :param reg_dict:替换词字典\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    for token_str,replac_str in reg_dict.items():\n",
    "        str_text = str_text.replace(token_str, replac_str)\n",
    "    return str_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seg_sentence(sentence, stop_words):\n",
    "    \"\"\"\n",
    "    对句子进行分词\n",
    "    :param sentence:句子，停用词\n",
    "    \"\"\"\n",
    "    sentence_seged = jieba.cut(sentence.strip())\n",
    "    word_list = [i for i in sentence_seged if i not in stop_words and i != ' ']\n",
    "    return word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_word(s1_train, s2_train, stopwords, spelling_corrections):\n",
    "\n",
    "    # 去除句子中的脱敏数字***，替换成一\n",
    "    re_object = re.compile(r'\\*+')\n",
    "\n",
    "    s1_all = []\n",
    "    s2_all = []\n",
    "    all = []\n",
    "\n",
    "    for s1_,s2_ in zip(s1_train, s2_train):\n",
    "        s1 = re_object.sub(u\"十一\", s1_)\n",
    "        s2 = re_object.sub(u\"十一\", s2_)\n",
    "        spell_corr_s1 = transform_other_word(s1, spelling_corrections)\n",
    "        spell_corr_s2 = transform_other_word(s2, spelling_corrections)\n",
    "\n",
    "        # 分词\n",
    "        seg_s1 = seg_sentence(spell_corr_s1, stopwords)\n",
    "        seg_s2 = seg_sentence(spell_corr_s2, stopwords)\n",
    "\n",
    "        all.extend(seg_s1)\n",
    "        all.extend(seg_s2)\n",
    "        s1_all.append(seg_s1)\n",
    "        s2_all.append(seg_s2)\n",
    "    source_list = []\n",
    "    # source_list = list(set(all))\n",
    "    source_list.append('<UNK>')\n",
    "    source_list.append('<PAD>')\n",
    "    source_list.extend(list(set(all)))\n",
    "    word2id = {}\n",
    "    id2word = {}\n",
    "    for index, char in enumerate(source_list):\n",
    "        word2id[char] = index\n",
    "        id2word[index] = char\n",
    "\n",
    "    return s1_all, s2_all, word2id, id2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_char(s1_train, s2_train, stopwords, spelling_corrections):\n",
    "\n",
    "    # 去除句子中的脱敏数字***，替换成一\n",
    "    re_object = re.compile(r'\\*+')\n",
    "\n",
    "    s1_all = []\n",
    "    s2_all = []\n",
    "    all = []\n",
    "    for s1_, s2_ in zip(s1_train, s2_train):\n",
    "        s1 = re_object.sub(u\"十一\", s1_)\n",
    "        s2 = re_object.sub(u\"十一\", s2_)\n",
    "        spell_corr_s1 = transform_other_word(s1, spelling_corrections)\n",
    "        spell_corr_s2 = transform_other_word(s2, spelling_corrections)\n",
    "        spell_corr_s1 = list(spell_corr_s1)\n",
    "        spell_corr_s2 = list(spell_corr_s2)\n",
    "        \n",
    "        all.extend(spell_corr_s1)\n",
    "        all.extend(spell_corr_s2)\n",
    "        split_s1 = [i for i in spell_corr_s1 if i not in stopwords and i.strip() != '']\n",
    "        split_s2 = [i for i in spell_corr_s2 if i not in stopwords and i.strip() != '']\n",
    "\n",
    "        s1_all.append(split_s1)\n",
    "        s2_all.append(split_s2)\n",
    "    source_list = []\n",
    "    # source_list = list(set(all))\n",
    "    source_list.append('<UNK>')\n",
    "    source_list.append('<PAD>')\n",
    "    source_list.extend(list(set(all)))\n",
    "    char2id = {}\n",
    "    id2char = {}\n",
    "    for index, char in enumerate(source_list):\n",
    "        char2id[char] = index\n",
    "        id2char[index] = char\n",
    "\n",
    "    return s1_all, s2_all, char2id, id2char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1_train = train_all[\"s1\"].tolist()\n",
    "s2_train = train_all[\"s2\"].tolist()\n",
    "y_train = train_all[\"label\"].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 获取对应的词表及词与id的映射"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1_word_all, s2_word_all, word2id, id2word = preprocessing_word(s1_train, s2_train, stopwords, spelling_corrections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1_char_all, s2_char_all, char2id, id2char = preprocessing_char(s1_train, s2_train, stopwords, spelling_corrections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_word2id(data, word2id):\n",
    "    data2id = []\n",
    "    for word_list in data:\n",
    "        id_list = [word2id.get(i) if word2id.get(i) is not None else word2id.get('<PAD>') for i in word_list]\n",
    "        data2id.append(id_list)\n",
    "    return data2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_data_set(s1_all, s2_all, word2id, y_train, max_l=25):\n",
    "    pad = word2id['<PAD>']\n",
    "    all_data = []\n",
    "    s1_data_id = make_word2id(s1_all, word2id)\n",
    "    s2_data_id = make_word2id(s2_all, word2id)\n",
    "    s1_all_new = []\n",
    "    s2_all_new = []\n",
    "    y = []\n",
    "    for i in range(len(s1_data_id)):\n",
    "        if len(s1_data_id[i]) > max_l:\n",
    "            s1_set = s1_data_id[i][:max_l]\n",
    "        else:\n",
    "            s1_set = np.concatenate((s1_data_id[i], np.tile(pad, max_l - len(s1_data_id[i]))), axis=0)\n",
    "        if len(s2_data_id[i]) > max_l:\n",
    "            s2_set = s2_data_id[i][:max_l]\n",
    "        else:\n",
    "            s2_set = np.concatenate((s2_data_id[i], np.tile(pad, max_l - len(s2_data_id[i]))), axis=0)\n",
    "        y_set = [1,0] if y_train[i] == 0 else [0,1]\n",
    "        s1_all_new.append(s1_set)\n",
    "        s2_all_new.append(s2_set)\n",
    "        y.append(y_set)\n",
    "    return s1_all_new, s2_all_new, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 获取id形式表示的文本特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1_word_id_all, s2_word_id_all, y_set = all_data_set(s1_word_all, s2_word_all, word2id, y_train, max_l=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1_char_id_all, s2_char_id_all, y_set = all_data_set(s1_char_all, s2_char_all, char2id, y_train, max_l=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将数据存到一个大列表里面，格式是[[s1,s2,y],[s1,s2,y],[s1,s2,y].......]\n",
    "all_data = []\n",
    "for i in range(len(s1_word_id_all)):\n",
    "        all_data.append([s1_word_id_all[i],s2_word_id_all[i],s1_char_id_all[i],s2_char_id_all[i],y_set[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio = int(len(all_data)*0.8)\n",
    "train_data = all_data[:ratio]\n",
    "test_data = all_data[ratio:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将数据存入pickle中\n",
    "with open(\"word_char_data.pk\", 'wb') as f1:\n",
    "    pickle.dump((train_data,test_data,word2id,id2word,char2id,id2char), f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
